# Upgrade Guide: v2.5 â†’ v2.6

## Quick Start

```bash
# 1. Update dependencies
pip install -r requirements.txt

# 2. Test with existing config (no changes needed)
python main.py configs/books_example.json

# 3. Observe new dashboard metrics
# You'll now see:
# - ğŸ“Š Total Entries (not just page count)
# - âš¡ Avg Entries/sec (extraction rate)
```

---

## New Features You Should Use

### 1. Enable Checkpointing for Large Scrapes

**Before** (v2.5): Crashes meant starting over from scratch

**After** (v2.6): Automatic resume from last successful URL

#### Update Your Config:
```json
{
  "name": "My Large Scrape",
  "use_checkpointing": true,  // ADD THIS LINE
  "base_url": "https://example.com"
}
```

#### How It Works:
```bash
# First run - processes 500 URLs, crashes at URL #347
python main.py configs/large_scrape.json
# Ctrl+C at any point

# Second run - automatically skips first 347 URLs
python main.py configs/large_scrape.json
# Starts from URL #348
```

---

### 2. Monitor Extraction Performance

**New Dashboard Metrics** (automatically enabled):

```
ğŸš€ Glider Scraper: My Project
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Metric                    â”‚ Value              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â±ï¸  Elapsed Time         â”‚ 00:03:42           â”‚
â”‚ âœ… Successful Pages      â”‚ 23                 â”‚
â”‚ âŒ Failed Pages          â”‚ 1                  â”‚
â”‚ ğŸ“Š Total Entries        â”‚ 4,892              â”‚  // NEW!
â”‚ âš¡ Avg Entries/sec       â”‚ 22.03              â”‚  // NEW!
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Use Case**: Identify bottlenecks
- Low RPS? Check network speed or increase `rate_limit`
- High RPS but low entries? Selectors might be wrong

---

### 3. Debug Browser Interactions

**Before** (v2.5): Silent failures, no visibility

**After** (v2.6): Detailed logs for every action

#### Check Interaction Logs:
```bash
# View interaction logs in real-time
tail -f logs/glider.log | grep "ğŸ®"
```

#### Example Output:
```
2025-12-22 23:15:42 | INFO | scraper:_handle_interactions:234 - ğŸ® Starting 4 browser interaction(s)...
2025-12-22 23:15:42 | DEBUG | scraper:_execute_interaction:312 -   [1/4] âœï¸  Filling '#search' with 'laptops'
2025-12-22 23:15:43 | DEBUG | scraper:_execute_interaction:320 -   [2/4] ğŸ‘† Clicking: button.search-btn
2025-12-22 23:15:45 | DEBUG | scraper:_execute_interaction:316 -   [3/4] â³ Waiting 2000ms...
2025-12-22 23:15:47 | DEBUG | scraper:_execute_interaction:324 -   [4/4] ğŸ“œ Scrolling to bottom...
2025-12-22 23:15:48 | INFO | scraper:_handle_interactions:248 - âœ… Interactions complete: 4 succeeded, 0 failed
```

---

### 4. Recover from Crashes

**Scenario**: Network drops mid-scrape

#### What Happens Now:
```bash
# Scraping 10,000 products...
python main.py configs/products.json

# Network dies at product #7,234
# Process crashes

# NO DATA LOSS!
# Check temporary file:
cat data/temp_stream.jsonl | wc -l
# Output: 7234 lines (all extracted data saved)

# Restart scraper:
python main.py configs/products.json
# Automatically resumes from product #7,235 (if checkpointing enabled)
```

---

## Troubleshooting

### Issue: High Memory Usage

**Cause**: Old version used unbounded hash set

**Fixed in v2.6**: Bloom filter uses constant 60KB regardless of dataset size

```bash
# Before: Memory grows with items
# 100k items = 6 MB
# 1M items = 60 MB

# After: Constant memory
# 100k items = 60 KB
# 1M items = 60 KB
```

---

### Issue: Data Lost on Ctrl+C

**Cause**: Old version only wrote after full page completion

**Fixed in v2.6**: Micro-batched writes save data every 10 items

```bash
# Old behavior:
# Page has 500 items, press Ctrl+C at item 247
# Result: 0 items saved

# New behavior:
# Page has 500 items, press Ctrl+C at item 247
# Result: 240 items saved (last batch at item 240)
```

---

### Issue: "Interaction failed" Warnings

**This is normal!** Interactions now retry once before giving up.

```bash
# Example log:
âš ï¸  [2/5] Interaction failed (click): Timeout
# Automatically retries...
âœ… [2/5] ğŸ‘† Clicking: button.load-more
```

**If all retries fail**: Scraper continues anyway (graceful degradation)

---

## Performance Tuning

### Adjust Batch Size (Advanced)

Default: 10 items per write

**For SSDs** (faster I/O):
```python
# In main.py, modify engine initialization:
engine = ScraperEngine(config, ...)
engine.batch_size = 50  # Larger batches = fewer writes
```

**For HDDs** (slower I/O):
```python
engine.batch_size = 5  # Smaller batches = more frequent saves
```

---

### Increase Bloom Filter Capacity

Default: 100k items

**For massive scrapes** (1M+ items):
```python
# In engine/scraper.py, line 88:
self.seen_hashes = BloomFilter(capacity=1000000, error_rate=0.001)
```

**Trade-off**: Higher capacity = more memory (but still <1MB)

---

## Verification Tests

### Test 1: Crash Recovery
```bash
# Start scrape
python main.py configs/books_example.json

# After 2-3 pages, press Ctrl+C

# Verify temp file exists
ls -lh data/temp_stream.jsonl

# Restart - should see "Skipped" count in dashboard
python main.py configs/books_example.json
```

**Expected**: Dashboard shows "Skipped (Checkpoint)" > 0

---

### Test 2: Entry Count Accuracy
```bash
# Scrape with known dataset
python main.py configs/books_example.json

# Compare dashboard "Total Entries" with final JSON
cat data/books_example_*.json | jq '.books | length'

# Should match!
```

---

### Test 3: Interaction Logging
```bash
# Use config with interactions
python main.py configs/quotes_js.json

# Check logs
grep "ğŸ®" logs/glider.log

# Should see detailed action logs
```

---

## Rollback Instructions

**If you encounter issues**, rollback to v2.5:

```bash
# Checkout previous version
git checkout main  # Before merge

# Reinstall old dependencies
pip install -r requirements.txt

# Your configs still work (backward compatible)
```

---

## FAQ

### Q: Do I need to update my configs?
**A**: No! All existing configs work without changes.

### Q: Will this slow down my scrapes?
**A**: Slightly (~10% I/O overhead), but crash safety is worth it.

### Q: What's the false positive rate for bloom filter?
**A**: 0.1% - meaning 1 in 1000 unique items might be incorrectly flagged as duplicate.

### Q: Can I disable the new features?
**A**: Bloom filter and micro-batching are always on. Checkpointing is optional (`use_checkpointing: true/false`).

### Q: How do I report bugs?
**A**: Open an issue on GitHub with:
- Config file
- Logs from `logs/glider.log`
- Expected vs actual behavior

---

## Summary

âœ… **Required**: Update dependencies (`pip install -r requirements.txt`)

âœ… **Recommended**: Enable checkpointing for large scrapes

âœ… **Optional**: Tune batch size for your storage type

ğŸ‰ **Enjoy**: Zero data loss, automatic recovery, better observability!
